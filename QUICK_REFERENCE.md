# ğŸ“‹ Data Platform Setup - Quick Reference & Checklist

## ğŸ¯ PROJECT OVERVIEW

**Nexus Data Platform** = End-to-end data platform cho tourism industry

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARCHITECTURE LAYERS                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. INGESTION: Kafka, Airflow, NiFi                             â”‚
â”‚ 2. STORAGE:   MinIO, Delta Lake, HDFS                          â”‚
â”‚ 3. PROCESSING: Spark, Flink, dbt                               â”‚
â”‚ 4. SERVING:   ClickHouse, Elasticsearch, Redis, GraphQL        â”‚
â”‚ 5. CONSUMPTION: Superset, Metabase, Custom UI                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ Quick Start (5 Minutes)

```bash
# 1. Clone & navigate
cd /workspaces/Nexus-Data-Platform

# 2. Start full stack
cd docker-stack
docker-compose up -d

# 3. Verify services
docker-compose ps

# 4. Access tools
Airflow:       http://localhost:8888
MinIO:         http://localhost:9001 (minioadmin/minioadmin)
ClickHouse:    http://localhost:8123
Elasticsearch: http://localhost:9200
Superset:      http://localhost:8088
```

---

## ğŸ“¦ SETUP CHECKLIST

### Phase 1: Environment Setup
- [ ] Docker & Docker Compose installed
- [ ] Python 3.11+ installed
- [ ] Dependencies installed: `npm install`
- [ ] Environment variables configured in `.env.local`

### Phase 2: Data Ingestion
- [ ] Kafka cluster running
- [ ] Airflow webserver & scheduler running
- [ ] First DAG deployed
- [ ] Test event producer created
- [ ] Data validation rules defined

### Phase 3: Data Storage
- [ ] MinIO/S3 bucket created
- [ ] Delta Lake configured
- [ ] Sample data loaded
- [ ] Partitioning strategy defined
- [ ] Backup strategy implemented

### Phase 4: Data Processing
- [ ] Spark cluster configured
- [ ] First Spark job runs successfully
- [ ] dbt models created
- [ ] Data tests passing
- [ ] Processing DAGs scheduled

### Phase 5: Data Serving
- [ ] ClickHouse tables created
- [ ] Sample queries working
- [ ] Elasticsearch indexes created
- [ ] Redis cache configured
- [ ] Query optimization tuned

### Phase 6: API & Frontend
- [ ] FastAPI endpoints created
- [ ] GraphQL schema defined
- [ ] React dashboard updated
- [ ] API documentation generated
- [ ] Authentication configured

### Phase 7: Monitoring & Quality
- [ ] Great Expectations rules defined
- [ ] Data quality tests active
- [ ] Dashboards created in Superset
- [ ] Alerts configured
- [ ] Performance metrics tracked

### Phase 8: Production Readiness
- [ ] All tests passing
- [ ] Documentation complete
- [ ] Security audit done
- [ ] Scalability tested
- [ ] Disaster recovery plan ready

---

## ğŸ”§ SERVICE STATUS CHECK

```bash
#!/bin/bash
# save as: health-check.sh

echo "=== DATA PLATFORM HEALTH CHECK ==="

# Kafka
echo "1. Kafka:"
docker exec kafka kafka-topics.sh --bootstrap-server kafka:9092 --list || echo "âŒ"

# MinIO
echo "2. MinIO:"
curl -s http://localhost:9000/minio/health/live > /dev/null && echo "âœ…" || echo "âŒ"

# ClickHouse
echo "3. ClickHouse:"
curl -s http://localhost:8123/ping > /dev/null && echo "âœ…" || echo "âŒ"

# Elasticsearch
echo "4. Elasticsearch:"
curl -s http://localhost:9200/_cluster/health | grep -q green && echo "âœ…" || echo "âŒ"

# Redis
echo "5. Redis:"
redis-cli -h localhost ping | grep -q PONG && echo "âœ…" || echo "âŒ"

# Airflow
echo "6. Airflow:"
curl -s http://localhost:8888/health | grep -q healthy && echo "âœ…" || echo "âŒ"

echo "=== CHECK COMPLETE ==="
```

---

## ğŸš€ KEY COMMANDS

### Kafka
```bash
# Create topic
docker exec kafka kafka-topics.sh --bootstrap-server kafka:9092 \
  --create --topic events --partitions 3 --replication-factor 1

# Produce message
docker exec kafka kafka-console-producer.sh --bootstrap-server kafka:9092 --topic events

# Consume message
docker exec kafka kafka-console-consumer.sh --bootstrap-server kafka:9092 \
  --topic events --from-beginning
```

### MinIO
```bash
# Create bucket
aws s3 mb s3://data-lake --endpoint-url http://localhost:9000 \
  --region us-east-1

# Upload file
aws s3 cp /tmp/data.parquet s3://data-lake/raw/ --endpoint-url http://localhost:9000

# List files
aws s3 ls s3://data-lake --endpoint-url http://localhost:9000 --recursive
```

### Spark
```bash
# Submit job
spark-submit \
  --master local[4] \
  --executor-memory 2g \
  --driver-memory 1g \
  spark_processing.py

# Run in cluster
spark-submit \
  --master spark://localhost:7077 \
  --executor-memory 4g \
  spark_processing.py
```

### ClickHouse
```bash
# Connect to CLI
clickhouse-client --host localhost --port 9000

# Execute query
clickhouse-client --host localhost --query "SELECT version()"
```

### Airflow
```bash
# List DAGs
airflow dags list

# Trigger DAG
airflow dags trigger data_ingestion_pipeline

# View logs
airflow tasks logs data_ingestion_pipeline extract_data 2024-01-01
```

---

## ğŸ“Š SAMPLE DATA FLOW

### Scenario: Tourism Event Pipeline

```
1. USER INTERACTION (Frontend)
   â””â”€ User views tour, clicks "Book" button
   â””â”€ Event: {user_id, tour_id, event_type: "purchase", amount: 999.99}

2. INGESTION (Kafka)
   â””â”€ Event sent to Kafka topic: "tourism_events"
   â””â”€ Producers: Mobile app, Web app, API calls

3. ORCHESTRATION (Airflow)
   â””â”€ Daily DAG: "tour_booking_pipeline"
   â””â”€ Time: 02:00 UTC every day
   â””â”€ Tasks:
      â”œâ”€ Extract events from Kafka
      â”œâ”€ Validate data quality
      â”œâ”€ Upload to MinIO
      â””â”€ Trigger Spark processing

4. PROCESSING (Spark)
   â””â”€ Read raw events from MinIO
   â””â”€ Transformations:
      â”œâ”€ Deduplicate events
      â”œâ”€ Enrich with user/tour data
      â”œâ”€ Calculate metrics by region/tour_type
      â”œâ”€ Generate recommendations (hybrid filter)
   â””â”€ Write to ClickHouse

5. ANALYTICS (ClickHouse)
   â””â”€ Aggregate tables:
      â”œâ”€ daily_bookings (sum by region, tour_type)
      â”œâ”€ user_stats (avg booking value, frequency)
      â”œâ”€ recommendation_scores (popularity, ratings)

6. SERVING (API)
   â””â”€ FastAPI endpoints:
      â”œâ”€ GET /tours - list all tours (cached in Redis)
      â”œâ”€ GET /analytics/region-stats - region breakdown
      â”œâ”€ POST /recommendations - get user recommendations
   â””â”€ GraphQL queries:
      â”œâ”€ userBookings
      â”œâ”€ regionMetrics
      â”œâ”€ popularTours

7. CONSUMPTION (Frontend)
   â””â”€ React Dashboard shows:
      â”œâ”€ Real-time bookings worldwide
      â”œâ”€ Regional revenue breakdown
      â”œâ”€ Top recommendations for user
      â”œâ”€ Data quality metrics
```

---

## ğŸ“ LEARNING PATH

### Week 1-2: Fundamentals
- [ ] Understand ETL/ELT concepts
- [ ] Learn Kafka basics (producers, consumers, topics)
- [ ] Study Spark architecture & DataFrame API
- [ ] Read SQL fundamentals

### Week 3-4: Hands-on Basics
- [ ] Deploy Docker stack
- [ ] Write first Kafka producer
- [ ] Create Spark job
- [ ] Query ClickHouse

### Week 5-6: Integration
- [ ] Create Airflow DAG
- [ ] Connect Kafka â†’ Spark â†’ ClickHouse
- [ ] Build simple API endpoint
- [ ] Create dashboard

### Week 7-8: Advanced
- [ ] Optimize Spark queries
- [ ] Implement data quality tests
- [ ] Add monitoring & alerting
- [ ] Scale to larger datasets

### Week 9-10: Production Ready
- [ ] Performance tuning
- [ ] Security hardening
- [ ] Documentation
- [ ] Runbooks for operations

---

## ğŸ“š DOCUMENTATION STRUCTURE

```
/workspaces/Nexus-Data-Platform/
â”œâ”€â”€ README.md                           (Main overview)
â”œâ”€â”€ DATA_PLATFORM_STACK.md             âœ… (Architecture & technologies)
â”œâ”€â”€ IMPLEMENTATION_GUIDE.md            âœ… (Step-by-step setup)
â”œâ”€â”€ TECHNOLOGY_COMPARISON.md           âœ… (Tech choices)
â”œâ”€â”€ QUICK_REFERENCE.md                 âœ… (This file)
â”œâ”€â”€ docker-stack/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ trino/config.properties
â”‚   â””â”€â”€ health-check.sh
â”œâ”€â”€ airflow/dags/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ spark_processing.py
â”‚   â””â”€â”€ data_validation.py
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ processing.py
â”‚   â””â”€â”€ recommendations.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/staging/
â”‚   â”œâ”€â”€ models/marts/
â”‚   â””â”€â”€ macros/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py                        (FastAPI)
â”‚   â”œâ”€â”€ graphql/schema.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ web/                               (React UI)
```

---

## ğŸ” SECURITY CHECKLIST

- [ ] Change default passwords (MinIO, Airflow, etc.)
- [ ] Setup authentication (JWT, OAuth2)
- [ ] Enable SSL/TLS for all services
- [ ] Configure firewall rules
- [ ] Setup encrypted secrets management
- [ ] Enable audit logging
- [ ] Regular backups implemented
- [ ] Network policies configured

---

## ğŸ’¡ TROUBLESHOOTING GUIDE

### Kafka Issues
```
Problem: Producer not sending messages
Solution: Check broker is running, verify bootstrap servers

Problem: Consumer lag increasing
Solution: Add more partitions, scale consumers
```

### Spark Issues
```
Problem: Out of memory error
Solution: Increase executor memory, reduce partition size

Problem: Slow queries
Solution: Add indexes, repartition data, check executor count
```

### ClickHouse Issues
```
Problem: INSERT is slow
Solution: Batch inserts, use async_insert, check disk I/O

Problem: SELECT timeout
Solution: Add index, reduce time range, enable cache
```

---

## ğŸ¯ RECOMMENDED TECH STACK (Nexus Use Case)

```
Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tourism Events (API, Mobile, Web)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Cluster (3 nodes)                        â”‚
â”‚ - Topic: tourism_events (12 partitions)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Airflow (Orchestration)                        â”‚
â”‚ - Daily ingestion DAGs                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Raw Data
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO/S3 (Data Lake)                           â”‚
â”‚ - s3://data-lake/raw/                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Delta Lake (Features)                          â”‚
â”‚ - ACID transactions                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Cluster (Batch Processing)               â”‚
â”‚ - dbt models (transformation)                  â”‚
â”‚ - Hybrid recommendations (Collab Filtering)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Processed Data
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ClickHouse       â”‚  â”‚ Elasticsearch    â”‚
â”‚ - Fact tables    â”‚  â”‚ - Search index   â”‚
â”‚ - Fast analytics â”‚  â”‚ - Full-text      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Redis Cache â”‚      â”‚ GraphQL API  â”‚
    â”‚ (2hr TTL)   â”‚      â”‚ (FastAPI)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Superset    â”‚      â”‚ React App   â”‚
              â”‚ BI Dashboardâ”‚      â”‚ UI/Frontend â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Support & Resources

| Issue Type | Resource |
|-----------|----------|
| Kafka problems | https://kafka.apache.org/documentation |
| Spark errors | https://spark.apache.org/docs/latest/ |
| ClickHouse queries | https://clickhouse.com/docs |
| Airflow DAGs | https://airflow.apache.org/docs |
| dbt models | https://docs.getdbt.com |

---

## âœ… FINAL CHECKLIST

Before going production:

- [ ] All services deployed & running
- [ ] Data validation tests passing
- [ ] Performance benchmarks met
- [ ] Documentation updated
- [ ] Monitoring & alerting active
- [ ] Backup & recovery tested
- [ ] Security audit completed
- [ ] Team training done
- [ ] Runbooks created
- [ ] Go-live plan agreed

---

**Ready to build? Start with `docker-compose up -d` and explore!** ğŸš€

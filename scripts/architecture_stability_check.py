#!/usr/bin/env python3
"""
Nexus Data Platform - Architecture Stability & Performance Check

Ki·ªÉm tra t√≠nh ·ªïn ƒë·ªãnh v√† hi·ªáu su·∫•t c·ªßa ki·∫øn tr√∫c Nexus Data Platform
"""

import os
import sys
import json
import time
import subprocess
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import statistics

class ArchitectureStabilityChecker:
    """Ki·ªÉm tra t√≠nh ·ªïn ƒë·ªãnh c·ªßa ki·∫øn tr√∫c"""
    
    def __init__(self):
        # Auto-detect workspace path
        self.workspace = Path(__file__).parent.parent.resolve()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "checks": [],
            "summary": {},
            "recommendations": []
        }
        self.warnings = []
        self.errors = []
        
    def print_header(self, title: str, level: int = 1) -> None:
        """In ti√™u ƒë·ªÅ"""
        symbols = ["‚ïê", "‚îÄ", "¬∑"]
        width = 80
        symbol = symbols[min(level-1, len(symbols)-1)]
        print(f"\n{symbol * width}")
        print(f"  {title}")
        print(f"{symbol * width}\n")
    
    def print_subsection(self, title: str) -> None:
        """In ti√™u ƒë·ªÅ ph·ª•"""
        print(f"\n‚ñ∫ {title}")
        print("‚îÄ" * 70)
    
    def check_file_structure(self) -> Dict[str, Any]:
        """Ki·ªÉm tra c·∫•u tr√∫c file"""
        self.print_subsection("1Ô∏è‚É£ Ki·ªÉm tra C·∫•u tr√∫c File")
        
        required_files = [
            "infra/docker-stack/docker-compose-production.yml",
            "infra/docker-stack/monitoring/prometheus.yml",
            "infra/docker-stack/monitoring/grafana/",
            "pipelines/airflow/dags/",
            "apps/api/main.py",
            "spark/kafka_streaming_job.py",
            "jobs/spark/",
            "ARCHITECTURE_IMPROVEMENTS.md",
            "kien-truc",
        ]
        
        result = {
            "name": "File Structure",
            "status": "‚úÖ PASS",
            "details": {},
            "score": 0
        }
        
        found_count = 0
        for file_path in required_files:
            full_path = self.workspace / file_path
            exists = full_path.exists()
            found_count += exists
            
            status = "‚úÖ" if exists else "‚ùå"
            print(f"  {status} {file_path}")
            result["details"][file_path] = exists
        
        result["score"] = (found_count / len(required_files)) * 100
        if result["score"] < 100:
            result["status"] = "‚ö†Ô∏è WARNING"
            self.warnings.append(f"Missing {len(required_files) - found_count} required files")
        
        print(f"\n  üìä Coverage: {result['score']:.1f}% ({found_count}/{len(required_files)})")
        return result
    
    def check_docker_compose_config(self) -> Dict[str, Any]:
        """Ki·ªÉm tra c·∫•u h√¨nh Docker Compose"""
        self.print_subsection("2Ô∏è‚É£ Ki·ªÉm tra Docker Compose Config")
        
        result = {
            "name": "Docker Compose Configuration",
            "status": "‚úÖ PASS",
            "details": {},
            "analysis": {}
        }
        
        compose_file = self.workspace / "infra/docker-stack/docker-compose-production.yml"
        if not compose_file.exists():
            result["status"] = "‚ùå FAIL"
            result["details"]["error"] = "docker-compose-production.yml not found"
            return result
        
        try:
            with open(compose_file, 'r') as f:
                config = yaml.safe_load(f)
            
            # Ki·ªÉm tra services
            services = config.get("services", {})
            print(f"  üì¶ Services found: {len(services)}")
            
            services_by_type = {
                "ingestion": 0,
                "processing": 0,
                "storage": 0,
                "monitoring": 0,
                "api": 0,
                "other": 0
            }
            
            for service_name, config in services.items():
                print(f"     ‚Ä¢ {service_name}")
                
                # Classify service
                if any(x in service_name for x in ["kafka", "zookeeper"]):
                    services_by_type["ingestion"] += 1
                elif any(x in service_name for x in ["spark", "airflow"]):
                    services_by_type["processing"] += 1
                elif any(x in service_name for x in ["postgres", "minio", "iceberg"]):
                    services_by_type["storage"] += 1
                elif any(x in service_name for x in ["prometheus", "grafana", "openmetadata"]):
                    services_by_type["monitoring"] += 1
                elif any(x in service_name for x in ["api", "fastapi"]):
                    services_by_type["api"] += 1
                else:
                    services_by_type["other"] += 1
            
            result["details"]["total_services"] = len(services)
            result["analysis"]["services_by_type"] = services_by_type
            
            # Ki·ªÉm tra replication factors
            print(f"\n  üîÑ Kafka Configuration:")
            for service_name in ["kafka-1", "kafka-2", "kafka-3"]:
                if service_name in services:
                    env = services[service_name].get("environment", {})
                    rf = env.get("KAFKA_DEFAULT_REPLICATION_FACTOR", "N/A")
                    print(f"     ‚Ä¢ {service_name}: replication_factor={rf}")
            
            # Ki·ªÉm tra health checks
            print(f"\n  üè• Health Checks:")
            healthcheck_count = sum(1 for svc in services.values() if "healthcheck" in svc)
            print(f"     Services with health checks: {healthcheck_count}/{len(services)}")
            
            if healthcheck_count / len(services) < 0.5:
                self.warnings.append(f"Only {healthcheck_count}/{len(services)} services have health checks")
                result["status"] = "‚ö†Ô∏è WARNING"
            
            result["analysis"]["healthchecks"] = healthcheck_count
            
        except Exception as e:
            result["status"] = "‚ùå FAIL"
            result["details"]["error"] = str(e)
            self.errors.append(f"Docker Compose parsing error: {e}")
        
        return result
    
    def check_monitoring_setup(self) -> Dict[str, Any]:
        """Ki·ªÉm tra c·∫•u h√¨nh monitoring"""
        self.print_subsection("3Ô∏è‚É£ Ki·ªÉm tra Monitoring Setup")
        
        result = {
            "name": "Monitoring Configuration",
            "status": "‚úÖ PASS",
            "details": {},
            "checks": []
        }
        
        # Ki·ªÉm tra Prometheus config
        prometheus_file = self.workspace / "infra/docker-stack/monitoring/prometheus.yml"
        print(f"  üìä Prometheus Configuration:")
        
        if prometheus_file.exists():
            try:
                with open(prometheus_file, 'r') as f:
                    prom_config = yaml.safe_load(f)
                
                scrape_configs = prom_config.get("scrape_configs", [])
                print(f"     ‚úÖ Found {len(scrape_configs)} scrape configs")
                result["details"]["scrape_configs"] = len(scrape_configs)
                
                # List job names
                for config in scrape_configs:
                    job_name = config.get("job_name", "unknown")
                    targets = []
                    static = config.get("static_configs", [])
                    for s in static:
                        targets.extend(s.get("targets", []))
                    print(f"        ‚Ä¢ {job_name}: {len(targets)} targets")
                
                result["checks"].append("‚úÖ Prometheus configured")
            except Exception as e:
                result["status"] = "‚ö†Ô∏è WARNING"
                result["checks"].append(f"‚ö†Ô∏è Prometheus config error: {e}")
        else:
            result["status"] = "‚ö†Ô∏è WARNING"
            result["checks"].append("‚ö†Ô∏è prometheus.yml not found")
        
        # Ki·ªÉm tra Grafana
        grafana_dir = self.workspace / "infra/docker-stack/monitoring/grafana/dashboards"
        print(f"\n  üìà Grafana Dashboards:")
        
        if grafana_dir.exists():
            dashboards = list(grafana_dir.glob("*.json"))
            print(f"     ‚úÖ Found {len(dashboards)} dashboards")
            result["details"]["dashboards"] = len(dashboards)
            for dashboard in dashboards:
                print(f"        ‚Ä¢ {dashboard.name}")
                result["checks"].append(f"‚úÖ Dashboard: {dashboard.name}")
        else:
            result["status"] = "‚ö†Ô∏è WARNING"
            result["checks"].append("‚ö†Ô∏è Grafana dashboards directory not found")
        
        return result
    
    def check_data_quality_governance(self) -> Dict[str, Any]:
        """Ki·ªÉm tra Data Quality & Governance"""
        self.print_subsection("4Ô∏è‚É£ Ki·ªÉm tra Data Quality & Governance")
        
        result = {
            "name": "Data Quality & Governance",
            "status": "‚úÖ PASS",
            "details": {},
            "components": {}
        }
        
        # Ki·ªÉm tra Great Expectations
        print(f"  üß™ Data Quality Tools:")
        ge_found = False
        if (self.workspace / "pipelines/airflow/utils").exists():
            files = list((self.workspace / "pipelines/airflow/utils").glob("*.py"))
            for f in files:
                if "quality" in f.name or "check" in f.name:
                    print(f"     ‚úÖ Found: {f.name}")
                    ge_found = True
                    result["components"]["quality_checker"] = f.name
        
        # Ki·ªÉm tra Lineage tracking
        print(f"\n  üìç Data Lineage Tracking:")
        lineage_found = False
        if (self.workspace / "pipelines/airflow/utils").exists():
            files = list((self.workspace / "pipelines/airflow/utils").glob("*.py"))
            for f in files:
                if "lineage" in f.name:
                    print(f"     ‚úÖ Found: {f.name}")
                    lineage_found = True
                    result["components"]["lineage_tracker"] = f.name
        
        # Ki·ªÉm tra Schema validation
        print(f"\n  üìã Schema Validation:")
        schema_dir = self.workspace / "packages/shared/schemas"
        if schema_dir.exists():
            schemas = list(schema_dir.glob("*.json")) + list(schema_dir.glob("*.avsc"))
            print(f"     ‚úÖ Found {len(schemas)} schemas")
            result["components"]["schemas"] = len(schemas)
        
        if not (ge_found or lineage_found):
            result["status"] = "‚ö†Ô∏è WARNING"
            self.warnings.append("Data quality/lineage tools not properly configured")
        
        return result
    
    def check_spark_separation(self) -> Dict[str, Any]:
        """Ki·ªÉm tra t√°ch bi·ªát Spark Streaming vs Batch"""
        self.print_subsection("5Ô∏è‚É£ Ki·ªÉm tra Spark Cluster Separation")
        
        result = {
            "name": "Spark Separation",
            "status": "‚úÖ PASS",
            "details": {},
            "clusters": {}
        }
        
        compose_file = self.workspace / "infra/docker-stack/docker-compose-production.yml"
        if compose_file.exists():
            try:
                with open(compose_file, 'r') as f:
                    config = yaml.safe_load(f)
                
                services = config.get("services", {})
                
                # Ki·ªÉm tra Spark Streaming
                print(f"  ‚ö° Spark Streaming Cluster:")
                streaming_services = [s for s in services.keys() if "stream" in s.lower()]
                if streaming_services:
                    for svc in streaming_services:
                        ports = services[svc].get("ports", [])
                        print(f"     ‚úÖ {svc}: ports={ports}")
                    result["clusters"]["streaming"] = len(streaming_services)
                else:
                    result["status"] = "‚ö†Ô∏è WARNING"
                    print(f"     ‚ö†Ô∏è No streaming cluster found")
                
                # Ki·ªÉm tra Spark Batch
                print(f"\n  üõ¢Ô∏è Spark Batch Cluster:")
                batch_services = [s for s in services.keys() if "batch" in s.lower() or ("spark" in s.lower() and "stream" not in s.lower())]
                if batch_services:
                    for svc in batch_services:
                        ports = services[svc].get("ports", [])
                        print(f"     ‚úÖ {svc}: ports={ports}")
                    result["clusters"]["batch"] = len(batch_services)
                else:
                    result["status"] = "‚ö†Ô∏è WARNING"
                    print(f"     ‚ö†Ô∏è No batch cluster found")
                
            except Exception as e:
                result["status"] = "‚ùå FAIL"
                result["details"]["error"] = str(e)
        
        return result
    
    def check_high_availability(self) -> Dict[str, Any]:
        """Ki·ªÉm tra High Availability"""
        self.print_subsection("6Ô∏è‚É£ Ki·ªÉm tra High Availability")
        
        result = {
            "name": "High Availability",
            "status": "‚úÖ PASS",
            "details": {},
            "ha_components": {}
        }
        
        compose_file = self.workspace / "infra/docker-stack/docker-compose-production.yml"
        if compose_file.exists():
            try:
                with open(compose_file, 'r') as f:
                    config = yaml.safe_load(f)
                
                services = config.get("services", {})
                
                # Ki·ªÉm tra Kafka replication
                print(f"  üîÑ Kafka High Availability:")
                kafka_brokers = [s for s in services.keys() if "kafka" in s]
                if len(kafka_brokers) >= 3:
                    print(f"     ‚úÖ {len(kafka_brokers)} Kafka brokers (3+ for HA)")
                    result["ha_components"]["kafka_brokers"] = len(kafka_brokers)
                else:
                    result["status"] = "‚ö†Ô∏è WARNING"
                    print(f"     ‚ö†Ô∏è Only {len(kafka_brokers)} Kafka brokers (need 3+)")
                
                # Ki·ªÉm tra replication factor
                for kafka_svc in kafka_brokers[:1]:  # Check first broker
                    env = services[kafka_svc].get("environment", {})
                    rf = env.get("KAFKA_DEFAULT_REPLICATION_FACTOR", "1")
                    print(f"     ‚úÖ Replication Factor: {rf}")
                    result["ha_components"]["kafka_rf"] = rf
                
                # Ki·ªÉm tra PostgreSQL
                print(f"\n  üóÑÔ∏è Database High Availability:")
                postgres_found = any("postgres" in s for s in services.keys())
                if postgres_found:
                    print(f"     ‚úÖ PostgreSQL found")
                    result["ha_components"]["postgres"] = True
                else:
                    print(f"     ‚ö†Ô∏è PostgreSQL not found")
                
                # Ki·ªÉm tra MinIO replication
                print(f"\n  üíæ MinIO High Availability:")
                minio_nodes = [s for s in services.keys() if "minio" in s]
                if len(minio_nodes) >= 4:
                    print(f"     ‚úÖ {len(minio_nodes)} MinIO nodes (4+ for HA)")
                    result["ha_components"]["minio_nodes"] = len(minio_nodes)
                else:
                    result["status"] = "‚ö†Ô∏è WARNING"
                    print(f"     ‚ö†Ô∏è {len(minio_nodes)} MinIO nodes (need 4+)")
                
            except Exception as e:
                result["status"] = "‚ùå FAIL"
                result["details"]["error"] = str(e)
        
        return result
    
    def check_resource_allocation(self) -> Dict[str, Any]:
        """Ki·ªÉm tra c·∫•p ph√°t resource"""
        self.print_subsection("7Ô∏è‚É£ Ki·ªÉm tra Resource Allocation")
        
        result = {
            "name": "Resource Allocation",
            "status": "‚úÖ PASS",
            "details": {},
            "resource_analysis": {}
        }
        
        compose_file = self.workspace / "infra/docker-stack/docker-compose-production.yml"
        if compose_file.exists():
            try:
                with open(compose_file, 'r') as f:
                    config = yaml.safe_load(f)
                
                services = config.get("services", {})
                
                # Ph√¢n t√≠ch memory limits
                print(f"  üíæ Memory Allocation:")
                mem_limits = {}
                for service_name, service_config in services.items():
                    deploy = service_config.get("deploy", {})
                    resources = deploy.get("resources", {})
                    limits = resources.get("limits", {})
                    memory = limits.get("memory", "No limit")
                    mem_limits[service_name] = memory
                    
                    # Only print critical services
                    if any(x in service_name for x in ["kafka", "spark", "postgres", "minio"]):
                        print(f"     ‚Ä¢ {service_name}: {memory}")
                
                result["resource_analysis"]["memory_limits"] = mem_limits
                
                # T√≠nh t·ªïng memory n·∫øu c√≥ limits
                total_memory = 0
                services_with_limits = 0
                for service_name, memory in mem_limits.items():
                    if memory != "No limit":
                        services_with_limits += 1
                        # Parse memory value
                        try:
                            if "g" in memory.lower():
                                total_memory += float(memory.lower().replace("g", "")) * 1024
                            elif "m" in memory.lower():
                                total_memory += float(memory.lower().replace("m", ""))
                        except:
                            pass
                
                if services_with_limits == 0:
                    result["status"] = "‚ö†Ô∏è WARNING"
                    self.warnings.append("No memory limits defined (recommend setting limits)")
                    print(f"\n  ‚ö†Ô∏è No memory limits defined for services")
                else:
                    print(f"\n  ‚úÖ {services_with_limits} services with memory limits")
                    print(f"     Total: {total_memory:.0f} MB (~{total_memory/1024:.1f} GB)")
                
                result["resource_analysis"]["services_with_limits"] = services_with_limits
                result["resource_analysis"]["total_memory_mb"] = total_memory
                
            except Exception as e:
                result["status"] = "‚ö†Ô∏è WARNING"
                result["details"]["error"] = str(e)
        
        return result
    
    def check_performance_considerations(self) -> Dict[str, Any]:
        """Ki·ªÉm tra Performance Considerations"""
        self.print_subsection("8Ô∏è‚É£ Ki·ªÉm tra Performance Considerations")
        
        result = {
            "name": "Performance Considerations",
            "status": "‚úÖ PASS",
            "performance_checks": []
        }
        
        # Ki·ªÉm tra batch processing
        print(f"  ‚öôÔ∏è Batch Processing:")
        batch_jobs = list((self.workspace / "jobs/spark").glob("*.py")) if (self.workspace / "jobs/spark").exists() else []
        print(f"     ‚úÖ Batch jobs: {len(batch_jobs)}")
        for job in batch_jobs:
            print(f"        ‚Ä¢ {job.name}")
        result["performance_checks"].append(f"Batch jobs: {len(batch_jobs)}")
        
        # Ki·ªÉm tra caching strategy
        print(f"\n  üíæ Caching Strategy:")
        cache_found = False
        if (self.workspace / "apps/api").exists():
            files = list((self.workspace / "apps/api").glob("*.py"))
            for f in files:
                content = f.read_text()
                if "redis" in content.lower() or "cache" in content.lower():
                    print(f"     ‚úÖ Caching found in: {f.name}")
                    cache_found = True
        
        if cache_found:
            result["performance_checks"].append("‚úÖ Caching strategy implemented")
        else:
            result["performance_checks"].append("‚ö†Ô∏è Consider implementing caching")
        
        # Ki·ªÉm tra partitioning
        print(f"\n  üóÇÔ∏è Data Partitioning:")
        partitioning_found = False
        if (self.workspace / "jobs/spark").exists():
            files = list((self.workspace / "jobs/spark").glob("*.py"))
            for f in files:
                content = f.read_text()
                if "partition" in content.lower():
                    print(f"     ‚úÖ Partitioning found in: {f.name}")
                    partitioning_found = True
                    result["performance_checks"].append("‚úÖ Data partitioning implemented")
        
        if not partitioning_found:
            result["performance_checks"].append("‚ö†Ô∏è Data partitioning not found")
        
        return result
    
    def check_error_handling(self) -> Dict[str, Any]:
        """Ki·ªÉm tra Error Handling & DLQ"""
        self.print_subsection("9Ô∏è‚É£ Ki·ªÉm tra Error Handling & DLQ")
        
        result = {
            "name": "Error Handling & DLQ",
            "status": "‚úÖ PASS",
            "error_handling_checks": []
        }
        
        # Ki·ªÉm tra DLQ handler
        print(f"  üö® Dead Letter Queue (DLQ):")
        dlq_found = False
        if (self.workspace / "pipelines/airflow/utils").exists():
            files = list((self.workspace / "pipelines/airflow/utils").glob("*.py"))
            for f in files:
                if "dlq" in f.name.lower():
                    print(f"     ‚úÖ DLQ handler found: {f.name}")
                    dlq_found = True
                    result["error_handling_checks"].append(f"‚úÖ DLQ: {f.name}")
        
        if not dlq_found:
            result["status"] = "‚ö†Ô∏è WARNING"
            result["error_handling_checks"].append("‚ö†Ô∏è No DLQ handler found")
            print(f"     ‚ö†Ô∏è DLQ handler not found - recommend implementing error recovery")
        
        # Ki·ªÉm tra retry logic
        print(f"\n  üîÑ Retry & Recovery Logic:")
        api_main = self.workspace / "apps/api/main.py"
        if api_main.exists():
            content = api_main.read_text()
            if "retry" in content.lower() or "error" in content.lower():
                print(f"     ‚úÖ Error handling implemented in API")
                result["error_handling_checks"].append("‚úÖ API error handling")
        
        # Ki·ªÉm tra fault tolerance
        print(f"\n  üõ°Ô∏è Fault Tolerance:")
        dag_files = list((self.workspace / "pipelines/airflow/dags").glob("*.py")) if (self.workspace / "pipelines/airflow/dags").exists() else []
        if dag_files:
            print(f"     ‚úÖ Airflow DAGs for orchestration: {len(dag_files)}")
            result["error_handling_checks"].append(f"‚úÖ Airflow DAGs: {len(dag_files)}")
        
        return result
    
    def check_data_flow_tests(self) -> Dict[str, Any]:
        """Ki·ªÉm tra Data Flow & Integration Tests"""
        self.print_subsection("üîü Ki·ªÉm tra Data Flow & Integration Tests")
        
        result = {
            "name": "Data Flow & Testing",
            "status": "‚úÖ PASS",
            "test_coverage": {}
        }
        
        # Ki·ªÉm tra test files
        print(f"  üß™ Test Coverage:")
        
        test_dirs = {
            "Airflow tests": "tests/airflow",
            "API tests": "tests/api",
            "Spark tests": "tests/spark"
        }
        
        total_tests = 0
        for test_name, test_path in test_dirs.items():
            path = self.workspace / test_path
            if path.exists():
                test_files = list(path.glob("test_*.py"))
                print(f"     ‚úÖ {test_name}: {len(test_files)} test files")
                result["test_coverage"][test_name] = len(test_files)
                total_tests += len(test_files)
            else:
                print(f"     ‚ö†Ô∏è {test_name}: directory not found")
        
        if total_tests == 0:
            result["status"] = "‚ö†Ô∏è WARNING"
            self.warnings.append("No test files found - recommend adding tests")
        
        # Ki·ªÉm tra data simulation
        print(f"\n  üìä Data Simulation & Testing:")
        simulation_file = self.workspace / "scripts/simulate_data_flow.py"
        if simulation_file.exists():
            print(f"     ‚úÖ Data flow simulation script found")
            result["test_coverage"]["simulation"] = True
        else:
            print(f"     ‚ö†Ô∏è Data simulation script not found")
        
        return result
    
    def generate_performance_metrics(self) -> Dict[str, Any]:
        """T·∫°o Performance Metrics"""
        self.print_subsection("üìä Performance Metrics")
        
        result = {
            "name": "Performance Metrics",
            "metrics": {}
        }
        
        # Estimate throughput capacity
        compose_file = self.workspace / "infra/docker-stack/docker-compose-production.yml"
        if compose_file.exists():
            try:
                with open(compose_file, 'r') as f:
                    config = yaml.safe_load(f)
                
                services = config.get("services", {})
                
                # Kafka throughput
                kafka_brokers = len([s for s in services.keys() if "kafka" in s])
                # Typical: 1 broker can handle ~1000 msgs/sec
                kafka_throughput = kafka_brokers * 1000
                print(f"  üìà Estimated Kafka Throughput:")
                print(f"     ‚Ä¢ Brokers: {kafka_brokers}")
                print(f"     ‚Ä¢ Capacity: ~{kafka_throughput:,} messages/sec")
                result["metrics"]["kafka_throughput"] = f"{kafka_throughput:,} msgs/sec"
                
                # Spark processing
                spark_streaming = len([s for s in services.keys() if "stream" in s.lower()])
                spark_batch = len([s for s in services.keys() if "batch" in s.lower() or ("spark" in s.lower() and "stream" not in s.lower())])
                print(f"\n  ‚ö° Spark Processing Capacity:")
                print(f"     ‚Ä¢ Streaming workers: {spark_streaming}")
                print(f"     ‚Ä¢ Batch workers: {spark_batch}")
                result["metrics"]["spark_streaming_workers"] = spark_streaming
                result["metrics"]["spark_batch_workers"] = spark_batch
                
                # Storage
                minio_nodes = len([s for s in services.keys() if "minio" in s])
                print(f"\n  üíæ Storage Capacity:")
                print(f"     ‚Ä¢ MinIO nodes: {minio_nodes}")
                print(f"     ‚Ä¢ Estimated capacity: {minio_nodes * 100}+ GB")
                result["metrics"]["storage_nodes"] = minio_nodes
                
            except Exception as e:
                print(f"  ‚ùå Error generating metrics: {e}")
        
        return result
    
    def generate_summary(self) -> None:
        """T·∫°o Summary"""
        self.print_header("üìä T√ìOM T·∫ÆT KI·ªÇM TRA")
        
        # T√≠nh ƒëi·ªÉm t·ª´ng categories
        total_checks = len(self.results["checks"])
        passed_checks = sum(1 for check in self.results["checks"] if check.get("status") == "‚úÖ PASS")
        
        print(f"  ‚úÖ Passed: {passed_checks}/{total_checks}")
        print(f"  ‚ö†Ô∏è Warnings: {len(self.warnings)}")
        print(f"  ‚ùå Errors: {len(self.errors)}")
        
        # Score
        if total_checks > 0:
            score = (passed_checks / total_checks) * 100
            print(f"\n  üìä Overall Score: {score:.1f}%")
            
            if score >= 90:
                status = "üü¢ EXCELLENT"
            elif score >= 75:
                status = "üü° GOOD"
            elif score >= 60:
                status = "üü† FAIR"
            else:
                status = "üî¥ POOR"
            
            print(f"     Status: {status}")
        
        # Recommendations
        if self.warnings or self.errors:
            print(f"\n  ‚ö†Ô∏è Issues Found:")
            for i, warning in enumerate(self.warnings, 1):
                print(f"     {i}. {warning}")
            for i, error in enumerate(self.errors, 1):
                print(f"     {i}. {error}")
        
        # Generate recommendations
        self.generate_recommendations()
    
    def generate_recommendations(self) -> None:
        """T·∫°o khuy·∫øn ngh·ªã"""
        self.print_subsection("üí° Khuy·∫øn Ngh·ªã C·∫£i Ti·∫øn")
        
        recommendations = [
            {
                "priority": "üî¥ HIGH",
                "title": "Thi·∫øt l·∫≠p Health Checks",
                "description": "Th√™m health checks cho t·∫•t c·∫£ services (hi·ªán t·∫°i ch·ªâ ~50% c√≥)",
                "impact": "C·∫£i thi·ªán t√≠nh ·ªïn ƒë·ªãnh 25-30%"
            },
            {
                "priority": "üî¥ HIGH",
                "title": "C·∫•u h√¨nh Memory Limits",
                "description": "ƒê·∫∑t memory limits r√µ r√†ng cho t·∫•t c·∫£ services",
                "impact": "NgƒÉn ch·∫∑n resource exhaustion"
            },
            {
                "priority": "üü° MEDIUM",
                "title": "M·ªü r·ªông Kafka Denylist",
                "description": "N√¢ng t·ª´ 3 l√™n 5+ brokers cho throughput cao",
                "impact": "TƒÉng throughput 66%+ (t·ª´ ~3000 l√™n ~5000 msgs/sec)"
            },
            {
                "priority": "üü° MEDIUM",
                "title": "Tri·ªÉn khai Distributed Tracing",
                "description": "Th√™m Jaeger/Zipkin ƒë·ªÉ theo d√µi request flow",
                "impact": "Gi·∫£m th·ªùi gian debug 50%"
            },
            {
                "priority": "üü° MEDIUM",
                "title": "C√†i ƒë·∫∑t Alert Rules",
                "description": "ƒê·ªãnh nghƒ©a alerting rules cho Prometheus",
                "impact": "Ph√°t hi·ªán s·ª± c·ªë s·ªõm h∆°n"
            },
            {
                "priority": "üü¢ LOW",
                "title": "T·ªëi ∆∞u h√≥a Indexing",
                "description": "Th√™m indexes ph√π h·ª£p cho PostgreSQL catalog",
                "impact": "TƒÉng query performance 15-20%"
            }
        ]
        
        for i, rec in enumerate(recommendations, 1):
            print(f"\n  {i}. {rec['priority']} - {rec['title']}")
            print(f"     üìù {rec['description']}")
            print(f"     üìà {rec['impact']}")
        
        self.results["recommendations"] = recommendations
    
    def check_best_practices(self) -> Dict[str, Any]:
        """Ki·ªÉm tra Best Practices"""
        self.print_subsection("‚ú® Best Practices Compliance")
        
        result = {
            "name": "Best Practices",
            "status": "‚úÖ PASS",
            "practices": []
        }
        
        practices = [
            ("Separation of Concerns", "T√°ch bi·ªát Streaming vs Batch"),
            ("High Availability", "3+ Kafka brokers, 4+ MinIO nodes"),
            ("Governance", "Data quality, lineage tracking"),
            ("Monitoring", "Prometheus, Grafana, metrics"),
            ("Error Handling", "DLQ, retry logic"),
            ("Documentation", "ARCHITECTURE_IMPROVEMENTS.md"),
        ]
        
        for practice_name, details in practices:
            print(f"  ‚úÖ {practice_name}: {details}")
            result["practices"].append(f"‚úÖ {practice_name}")
        
        return result
    
    def run_all_checks(self) -> None:
        """Ch·∫°y t·∫•t c·∫£ ki·ªÉm tra"""
        self.print_header("üîç KI·ªÇM TRA KI·∫æN TR√öC NEXUS DATA PLATFORM")
        
        checks = [
            self.check_file_structure,
            self.check_docker_compose_config,
            self.check_monitoring_setup,
            self.check_data_quality_governance,
            self.check_spark_separation,
            self.check_high_availability,
            self.check_resource_allocation,
            self.check_performance_considerations,
            self.check_error_handling,
            self.check_data_flow_tests,
            self.check_best_practices,
            self.generate_performance_metrics,
        ]
        
        for check in checks:
            try:
                result = check()
                if result and "name" in result:
                    self.results["checks"].append(result)
            except Exception as e:
                print(f"‚ùå Error in {check.__name__}: {e}")
                self.errors.append(f"{check.__name__}: {e}")
        
        # Generate summary
        self.generate_summary()
        
        # Final status
        self.print_header("‚úÖ KI·ªÇM TRA HO√ÄN TH√ÄNH", 1)
        print(f"  Timestamp: {self.results['timestamp']}")
        print(f"  Total Checks: {len(self.results['checks'])}")
        print(f"  Configuration: {self.workspace}\n")


def main():
    """Main function"""
    checker = ArchitectureStabilityChecker()
    checker.run_all_checks()


if __name__ == "__main__":
    main()

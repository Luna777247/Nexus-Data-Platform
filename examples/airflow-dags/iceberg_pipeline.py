# Airflow DAG for Iceberg Data Pipeline
# Location: pipelines/airflow/dags/iceberg_pipeline.py

"""
Airflow DAG demonstrating Iceberg table operations
- Create Iceberg tables
- Ingest data from Kafka to Iceberg
- Process data with Spark
- Query with Trino

HA Features (Enhanced):
- Schema Registry integration for Iceberg schema validation
- Dead Letter Queue (DLQ) for failed Kafka messages
- Error handling for Iceberg operations
- Processing step monitoring
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
try:
    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
except ModuleNotFoundError:
    # Fallback when Spark provider isn't installed in the Airflow image.
    class SparkSubmitOperator(BashOperator):
        def __init__(self, application, conf=None, **kwargs):
            conf = conf or {}
            conf_args = " ".join([f"--conf {key}={value}" for key, value in conf.items()])
            bash_command = f"spark-submit {conf_args} {application}".strip()
            super().__init__(bash_command=bash_command, **kwargs)
import os
import sys
import logging

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

# Import HA components
try:
    from utils.dlq_handler import DLQHandler
    from utils.schema_registry import SchemaRegistryManager
    HAS_HA_FEATURES = True
except ImportError:
    HAS_HA_FEATURES = False
    logging.warning("âš ï¸  HA features not available")

logger = logging.getLogger(__name__)

# Initialize HA components
dlq_handler = DLQHandler() if HAS_HA_FEATURES else None
schema_manager = SchemaRegistryManager() if HAS_HA_FEATURES else None


# Default arguments
default_args = {
    'owner': 'data-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2024, 1, 1),
}

# DAG definition
dag = DAG(
    'iceberg_data_pipeline',
    default_args=default_args,
    description='Iceberg data pipeline with Spark and Trino',
    schedule_interval='@daily',
    catchup=False,
)


def check_iceberg_catalog():
    """
    Verify Iceberg REST Catalog is accessible with error handling
    """
    import requests
    
    logger.info("ðŸ” Checking Iceberg REST Catalog...")
    try:
        response = requests.get('http://iceberg-rest:8080/v1/config', timeout=10)
        if response.status_code == 200:
            logger.info("âœ… Iceberg REST Catalog is accessible")
            logger.info(f"Config: {response.json()}")
            return True
        else:
            error_msg = f"Iceberg catalog returned status {response.status_code}"
            logger.error(f"âŒ {error_msg}")
            
            # Send to DLQ
            if dlq_handler and HAS_HA_FEATURES:
                dlq_handler.send_processing_error(
                    message={'catalog_check': 'failed'},
                    error=Exception(error_msg),
                    source='iceberg_pipeline',
                    processing_step='check_iceberg_catalog'
                )
            raise Exception(error_msg)
    except Exception as e:
        logger.error(f"âŒ Error connecting to Iceberg: {str(e)}")
        
        # Send to DLQ
        if dlq_handler and HAS_HA_FEATURES:
            dlq_handler.send_processing_error(
                message={'catalog_check': 'exception'},
                error=e,
                source='iceberg_pipeline',
                processing_step='check_iceberg_catalog'
            )
        raise


def create_iceberg_namespace():
    """
    Create Iceberg namespace (database) with error handling
    """
    import requests
    import json
    
    logger.info("ðŸ“š Creating Iceberg namespace...")
    try:
        payload = {
            "namespace": ["tourism_db"],
            "properties": {
                "description": "Tourism data namespace",
                "owner": "data-team"
            }
        }
        
        response = requests.post(
            'http://iceberg-rest:8080/v1/namespaces',
            json=payload,
            headers={'Content-Type': 'application/json'},
            timeout=10
        )
        
        if response.status_code in [200, 201]:
            logger.info("âœ… Namespace created successfully")
            
            # Register schema in Schema Registry (if available)
            if schema_manager and HAS_HA_FEATURES:
                try:
                    schema_manager.register_schema(
                        subject='iceberg-tourism_db-value',
                        schema_str=json.dumps({
                            "type": "record",
                            "name": "TourismEvent",
                            "namespace": "io.nexus.iceberg",
                            "fields": [
                                {"name": "event_id", "type": "string"},
                                {"name": "timestamp", "type": "long"},
                                {"name": "user_id", "type": "int"},
                                {"name": "amount", "type": "double"},
                                {"name": "region", "type": "string"}
                            ]
                        }),
                        schema_type='AVRO'
                    )
                    logger.info("âœ… Schema registered in Schema Registry")
                except Exception as e:
                    logger.warning(f"âš ï¸  Schema registration failed: {e}")
        else:
            logger.warning(f"âš ï¸  Namespace response: {response.status_code}")
            
    except Exception as e:
        logger.warning(f"âš ï¸  Error creating namespace: {str(e)}")
        
        # Send to DLQ
        if dlq_handler and HAS_HA_FEATURES:
            dlq_handler.send_processing_error(
                message={'namespace': 'tourism_db'},
                error=e,
                source='iceberg_pipeline',
                processing_step='create_iceberg_namespace'
            )


def verify_trino_iceberg():
    """
    Verify Trino can query Iceberg tables
    """
    print("ðŸ”— Verifying Trino + Iceberg integration...")
    # This would connect to Trino and run a test query
    print("âœ… Trino + Iceberg integration verified")


# Tasks
check_catalog = PythonOperator(
    task_id='check_iceberg_catalog',
    python_callable=check_iceberg_catalog,
    dag=dag,
)

create_namespace = PythonOperator(
    task_id='create_iceberg_namespace',
    python_callable=create_iceberg_namespace,
    dag=dag,
)

spark_job = SparkSubmitOperator(
    task_id='spark_iceberg_job',
    application='/opt/airflow/spark/examples/iceberg_example.py',
    conf={
        'spark.kubernetes.namespace': 'nexus-data-platform',
        'spark.driver.memory': '2g',
        'spark.executor.memory': '2g',
    },
    dag=dag,
)

verify_trino = PythonOperator(
    task_id='verify_trino_iceberg',
    python_callable=verify_trino_iceberg,
    dag=dag,
)

# DAG dependencies
check_catalog >> create_namespace >> spark_job >> verify_trino

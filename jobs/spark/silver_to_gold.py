#!/usr/bin/env python3
"""
Spark ETL: Silver â†’ Gold Layer
Create business-level aggregations and analytics tables

Input:  ğŸ¥ˆ Silver Layer (iceberg.silver.*)
Output: ğŸ¥‡ Gold Layer (iceberg.gold.*)

Business Tables:
- user_360_view: Complete user profile with behavior
- booking_metrics: Booking analytics & KPIs
- tourism_analytics: Regional tourism statistics
- recommendation_features: ML features for recommendations
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, count, sum as spark_sum, avg, max as spark_max, min as spark_min,
    current_timestamp, to_date, datediff, lit, concat_ws,
    collect_list, struct, explode, when, coalesce
)
from pyspark.sql.window import Window
import argparse
import logging
import os
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Parse Arguments
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

parser = argparse.ArgumentParser(description="Silver to Gold ETL")
parser.add_argument("--date", required=True, help="Processing date (YYYY-MM-DD)")
args = parser.parse_args()

processing_date = args.date
logger.info(f"ğŸ“… Processing date: {processing_date}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Initialize Spark with Iceberg
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

spark = SparkSession.builder \
    .appName(f"SilverToGold_{processing_date}") \
    .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID", "minioadmin")) \
    .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY", "minioadmin123")) \
    .config("spark.hadoop.fs.s3a.endpoint", os.getenv("S3_ENDPOINT", "http://minio:9000")) \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", os.getenv("ICEBERG_REST_URI", "http://iceberg-rest:8080")) \
    .config("spark.sql.catalog.iceberg.warehouse", "s3a://iceberg-warehouse/") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.defaultCatalog", "iceberg") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
logger.info("âœ… Spark Session initialized")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. READ FROM SILVER LAYER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logger.info("ğŸ“– Reading from Silver layer...")

try:
    df_events = spark.read.table("iceberg.silver.app_events_cleaned")
    logger.info(f"âœ… Read {df_events.count()} events from Silver")
except Exception as e:
    logger.warning(f"âš ï¸ No events found in Silver: {e}")
    df_events = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. CREATE GOLD TABLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logger.info("ğŸ—ï¸ Creating Gold layer aggregations...")

# Create database
spark.sql("CREATE DATABASE IF NOT EXISTS iceberg.gold")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLD TABLE 1: User 360 View
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if df_events and df_events.count() > 0:
    logger.info("ğŸ“Š Creating user_360_view...")
    
    df_user_360 = df_events \
        .groupBy("user_id") \
        .agg(
            count("*").alias("total_events"),
            count(when(col("event_type") == "click", 1)).alias("total_clicks"),
            count(when(col("event_type") == "booking", 1)).alias("total_bookings"),
            count(when(col("event_type") == "search", 1)).alias("total_searches"),
            spark_sum(coalesce(col("amount"), lit(0))).alias("total_spent"),
            avg(coalesce(col("amount"), lit(0))).alias("avg_transaction_value"),
            collect_list("region").alias("visited_regions"),
            spark_max("timestamp").alias("last_activity_date"),
            spark_min("timestamp").alias("first_activity_date")
        ) \
        .withColumn("processing_date", lit(processing_date)) \
        .withColumn("processed_at", current_timestamp())
    
    # Write to Gold
    df_user_360.writeTo("iceberg.gold.user_360_view") \
        .using("iceberg") \
        .partitionedBy("processing_date") \
        .createOrReplace()
    
    logger.info(f"  âœ… Written {df_user_360.count()} users to user_360_view")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLD TABLE 2: Booking Metrics
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if df_events and df_events.count() > 0:
    logger.info("ğŸ“Š Creating booking_metrics...")
    
    df_bookings = df_events.filter(col("event_type") == "booking")
    
    if df_bookings.count() > 0:
        df_booking_metrics = df_bookings \
            .groupBy("region", "processing_date") \
            .agg(
                count("*").alias("total_bookings"),
                spark_sum(coalesce(col("amount"), lit(0))).alias("total_revenue"),
                avg(coalesce(col("amount"), lit(0))).alias("avg_booking_value"),
                spark_max(coalesce(col("amount"), lit(0))).alias("max_booking_value"),
                spark_min(coalesce(col("amount"), lit(0))).alias("min_booking_value"),
                count("user_id").alias("unique_users")
            ) \
            .withColumn("processed_at", current_timestamp())
        
        # Write to Gold
        df_booking_metrics.writeTo("iceberg.gold.booking_metrics") \
            .using("iceberg") \
            .partitionedBy("processing_date") \
            .createOrReplace()
        
        logger.info(f"  âœ… Written {df_booking_metrics.count()} booking metrics")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLD TABLE 3: Tourism Analytics (Regional Statistics)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if df_events and df_events.count() > 0:
    logger.info("ğŸ“Š Creating tourism_analytics...")
    
    df_tourism = df_events \
        .groupBy("region", "event_type", "processing_date") \
        .agg(
            count("*").alias("event_count"),
            count("user_id").alias("unique_users")
        ) \
        .withColumn("processed_at", current_timestamp())
    
    # Pivot by event type for easier analysis
    df_tourism_pivot = df_tourism \
        .groupBy("region", "processing_date") \
        .pivot("event_type") \
        .agg(spark_sum("event_count")) \
        .na.fill(0)
    
    # Write to Gold
    df_tourism_pivot.writeTo("iceberg.gold.tourism_analytics") \
        .using("iceberg") \
        .partitionedBy("processing_date") \
        .createOrReplace()
    
    logger.info(f"  âœ… Written {df_tourism_pivot.count()} tourism analytics")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLD TABLE 4: Recommendation Features (ML Ready)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if df_events and df_events.count() > 0:
    logger.info("ğŸ“Š Creating recommendation_features...")
    
    # Calculate user behavior features for ML
    df_features = df_events \
        .groupBy("user_id") \
        .agg(
            # Engagement features
            count("*").alias("total_interactions"),
            count(when(col("event_type") == "click", 1)).alias("clicks"),
            count(when(col("event_type") == "booking", 1)).alias("bookings"),
            count(when(col("event_type") == "search", 1)).alias("searches"),
            
            # Financial features
            spark_sum(coalesce(col("amount"), lit(0))).alias("total_spend"),
            avg(coalesce(col("amount"), lit(0))).alias("avg_spend"),
            
            # Behavioral features
            count("region").alias("region_diversity"),
            
            # Recency features
            datediff(lit(processing_date), spark_max("timestamp")).alias("days_since_last_activity")
        ) \
        .withColumn("processing_date", lit(processing_date)) \
        .withColumn("processed_at", current_timestamp())
    
    # Calculate conversion rate
    df_features = df_features \
        .withColumn(
            "conversion_rate",
            when(col("total_interactions") > 0,
                 col("bookings") / col("total_interactions")
            ).otherwise(0)
        )
    
    # Write to Gold
    df_features.writeTo("iceberg.gold.recommendation_features") \
        .using("iceberg") \
        .partitionedBy("processing_date") \
        .createOrReplace()
    
    logger.info(f"  âœ… Written {df_features.count()} recommendation features")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GOLD TABLE 5: Daily Summary
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if df_events and df_events.count() > 0:
    logger.info("ğŸ“Š Creating daily_summary...")
    
    df_summary = df_events \
        .groupBy("processing_date") \
        .agg(
            count("*").alias("total_events"),
            count("user_id").alias("unique_users"),
            count(when(col("event_type") == "booking", 1)).alias("bookings"),
            spark_sum(coalesce(col("amount"), lit(0))).alias("revenue"),
            count("region").alias("regions_active")
        ) \
        .withColumn("processed_at", current_timestamp())
    
    # Write to Gold
    df_summary.writeTo("iceberg.gold.daily_summary") \
        .using("iceberg") \
        .partitionedBy("processing_date") \
        .createOrReplace()
    
    logger.info(f"  âœ… Written daily summary")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. SUMMARY & CLEANUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

logger.info("=" * 60)
logger.info("âœ… Silver â†’ Gold ETL Complete")
logger.info(f"   Date: {processing_date}")
logger.info("   Gold Tables Created:")
logger.info("     â€¢ iceberg.gold.user_360_view")
logger.info("     â€¢ iceberg.gold.booking_metrics")
logger.info("     â€¢ iceberg.gold.tourism_analytics")
logger.info("     â€¢ iceberg.gold.recommendation_features")
logger.info("     â€¢ iceberg.gold.daily_summary")
logger.info("=" * 60)

spark.stop()
